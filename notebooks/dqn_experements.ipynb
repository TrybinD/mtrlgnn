{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты с DQN для комбинаторных задач"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4co.envs.common.base import RL4COEnvBase\n",
    "\n",
    "from rl4co.models.zoo.l2d.policy import L2DPolicy\n",
    "\n",
    "from typing import IO, Any, Optional, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lightning.fabric.utilities.types import _MAP_LOCATION_TYPE, _PATH\n",
    "from lightning.pytorch.core.saving import _load_from_checkpoint\n",
    "from tensordict import TensorDict\n",
    "from typing_extensions import Self\n",
    "\n",
    "from rl4co.envs.common.base import RL4COEnvBase\n",
    "from rl4co.models.rl.common.base import RL4COLitModule\n",
    "from rl4co.utils.lightning import get_lightning_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(RL4COLitModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: RL4COEnvBase,\n",
    "        policy: nn.Module,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(env, policy, **kwargs)\n",
    "        self.policy = policy\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "    def shared_step(\n",
    "        self, batch: Any, batch_idx: int, phase: str, dataloader_idx: int = None\n",
    "    ):\n",
    "        td = self.env.reset(batch)\n",
    "        # Perform forward pass (i.e., constructing solution and computing q values)\n",
    "        out = self.policy(td, self.env, phase=phase, select_best=phase != \"train\")\n",
    "\n",
    "        # Compute loss\n",
    "        if phase == \"train\":\n",
    "            out = self.calculate_loss(out)\n",
    "\n",
    "        metrics = self.log_metrics(out, phase, dataloader_idx=dataloader_idx)\n",
    "        return {\"loss\": out.get(\"loss\", None), **metrics}\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        policy_out: dict,\n",
    "    ):\n",
    "        \n",
    "        predicted_q_values = policy_out[\"predicted_q_values\"]\n",
    "        actions = policy_out[\"actions\"]\n",
    "        rewards = policy_out[\"rewards\"]\n",
    "\n",
    "        fact_q_values = make_fact_q_values(predicted_q_values, rewards, actions)\n",
    "\n",
    "\n",
    "        # Main loss function\n",
    "        loss = self.loss(predicted_q_values, fact_q_values)\n",
    "        policy_out.update(\n",
    "            {\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "        )\n",
    "        return policy_out\n",
    "\n",
    "    def set_decode_type_multistart(self, phase: str):\n",
    "        \"\"\"Set decode type to `multistart` for train, val and test in policy.\n",
    "        For example, if the decode type is `greedy`, it will be set to `multistart_greedy`.\n",
    "\n",
    "        Args:\n",
    "            phase: Phase to set decode type for. Must be one of `train`, `val` or `test`.\n",
    "        \"\"\"\n",
    "        attribute = f\"{phase}_decode_type\"\n",
    "        attr_get = getattr(self.policy, attribute)\n",
    "        # If does not exist, log error\n",
    "        if attr_get is None:\n",
    "            return\n",
    "        elif \"multistart\" in attr_get:\n",
    "            return\n",
    "        else:\n",
    "            setattr(self.policy, attribute, f\"multistart_{attr_get}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path: _PATH | IO,\n",
    "        map_location: _MAP_LOCATION_TYPE = None,\n",
    "        hparams_file: Optional[_PATH] = None,\n",
    "        strict: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Self:\n",
    "        \"\"\"Load model from checkpoint/\n",
    "\n",
    "        Note:\n",
    "            This is a modified version of `load_from_checkpoint` from `pytorch_lightning.core.saving`.\n",
    "            It deals with matching keys for the baseline by first running setup\n",
    "        \"\"\"\n",
    "\n",
    "        if strict:\n",
    "            strict = False\n",
    "\n",
    "        # Do not use strict\n",
    "        loaded = _load_from_checkpoint(\n",
    "            cls,\n",
    "            checkpoint_path,\n",
    "            map_location,\n",
    "            hparams_file,\n",
    "            strict,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return cast(Self, loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2DPolicyDQN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        env_name: str = \"tsp\",\n",
    "        epsilon: float = 0.05,\n",
    "        train_decode_type: str = \"sampling\",\n",
    "        val_decode_type: str = \"greedy\",\n",
    "        test_decode_type: str = \"greedy\",\n",
    "        **unused_kw,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # if len(unused_kw) > 0:\n",
    "            # log.error(f\"Found {len(unused_kw)} unused kwargs: {unused_kw}\")\n",
    "\n",
    "        self.env_name = env_name\n",
    "\n",
    "        # Encoder and decoder\n",
    "        # if encoder is None:\n",
    "            # log.warning(\"`None` was provided as encoder. Using `NoEncoder`.\")\n",
    "            # encoder = NoEncoder()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # Decoding strategies\n",
    "        self.epsilon = epsilon\n",
    "        self.train_decode_type = train_decode_type\n",
    "        self.val_decode_type = val_decode_type\n",
    "        self.test_decode_type = test_decode_type\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        td: TensorDict,\n",
    "        env: Optional[str | RL4COEnvBase] = None,\n",
    "        phase: str = \"train\",\n",
    "        calc_reward: bool = True,\n",
    "        return_actions: bool = True,\n",
    "        max_steps=1_000_000,\n",
    "        **decoding_kwargs,\n",
    "    ) -> dict:\n",
    "        \"\"\"Forward pass of the policy.\n",
    "\n",
    "        Args:\n",
    "            td: TensorDict containing the environment state\n",
    "            env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n",
    "                it is more efficient to pass an already instantiated environment each time for fine-grained control\n",
    "            phase: Phase of the algorithm (train, val, test)\n",
    "            calc_reward: Whether to calculate the reward\n",
    "            return_actions: Whether to return the actions\n",
    "            return_entropy: Whether to return the entropy\n",
    "            return_hidden: Whether to return the hidden state\n",
    "            return_init_embeds: Whether to return the initial embeddings\n",
    "            return_sum_log_likelihood: Whether to return the sum of the log likelihood\n",
    "            actions: Actions to use for evaluating the policy.\n",
    "                If passed, use these actions instead of sampling from the policy to calculate log likelihood\n",
    "            max_steps: Maximum number of decoding steps for sanity check to avoid infinite loops if envs are buggy (i.e. do not reach `done`)\n",
    "            decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n",
    "\n",
    "        Returns:\n",
    "            out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoder: get encoder output and initial embeddings from initial state\n",
    "        hidden, init_embeds = self.encoder(td)\n",
    "\n",
    "        # Instantiate environment if needed\n",
    "        # if isinstance(env, str) or env is None:\n",
    "        #     env_name = self.env_name if env is None else env\n",
    "        #     log.info(f\"Instantiated environment not provided; instantiating {env_name}\")\n",
    "        #     env = get_env(env_name)\n",
    "\n",
    "        # Get decode type depending on phase and whether actions are passed for evaluation\n",
    "        # decode_type = decoding_kwargs.pop(\"decode_type\", None)\n",
    "        # if actions is not None:\n",
    "        #     decode_type = \"evaluate\"\n",
    "        # elif decode_type is None:\n",
    "        #     decode_type = getattr(self, f\"{phase}_decode_type\")\n",
    "\n",
    "        # Setup decoding strategy\n",
    "        # we pop arguments that are not part of the decoding strategy\n",
    "        # decode_strategy: DecodingStrategy = get_decoding_strategy(\n",
    "        #     decode_type,\n",
    "        #     temperature=decoding_kwargs.pop(\"temperature\", self.temperature),\n",
    "        #     tanh_clipping=decoding_kwargs.pop(\"tanh_clipping\", self.tanh_clipping),\n",
    "        #     mask_logits=decoding_kwargs.pop(\"mask_logits\", self.mask_logits),\n",
    "        #     store_all_logp=decoding_kwargs.pop(\"store_all_logp\", return_entropy),\n",
    "        #     **decoding_kwargs,\n",
    "        # )\n",
    "\n",
    "        # Pre-decoding hook: used for the initial step(s) of the decoding strategy\n",
    "        # td, env, num_starts = decode_strategy.pre_decoder_hook(td, env)\n",
    "\n",
    "        # Additionally call a decoder hook if needed before main decoding\n",
    "        # td, env, hidden = self.decoder.pre_decoder_hook(td, env, hidden, num_starts)\n",
    "\n",
    "        # Main decoding: loop until all sequences are done\n",
    "        step = 0\n",
    "        predicted_q_values = []\n",
    "        masks = []\n",
    "        done_padding = []\n",
    "        selected_actions = []\n",
    "\n",
    "        while not td[\"done\"].all():\n",
    "            q_values, mask = self.decoder(td, *hidden)\n",
    "            # td = decode_strategy.step(\n",
    "            #     logits,\n",
    "            #     mask,\n",
    "            #     td,\n",
    "            #     action=actions[..., step] if actions is not None else None,\n",
    "            # )\n",
    "\n",
    "            predicted_q_values.append(q_values)\n",
    "            done_padding.append(td[\"done\"])\n",
    "\n",
    "            epsilon = self.epsilon if phase == \"train\" else 0.0\n",
    "            bs = q_values.size(0)\n",
    "            device = q_values.device\n",
    "\n",
    "            # 1. Жадные действия (выбираем max Q среди допустимых)\n",
    "            masked_q = q_values.masked_fill(~mask, -float(\"inf\"))  # (bs, n_actions)\n",
    "            greedy_actions = masked_q.argmax(dim=-1)  # (bs,)\n",
    "\n",
    "            # 2. Случайные допустимые действия\n",
    "            random_actions = torch.zeros(bs, dtype=torch.long, device=device)\n",
    "            for i in range(bs):\n",
    "                valid_actions = mask[i].nonzero().squeeze(-1)  # Допустимые действия для i-го элемента\n",
    "                random_actions[i] = valid_actions[torch.randint(len(valid_actions), (1,))]\n",
    "\n",
    "            # 3. Для каждого элемента батча решаем: greedy или random\n",
    "            use_random = torch.rand(bs, device=device) < epsilon  # (bs,) bool\n",
    "            selected_action = torch.where(use_random, random_actions, greedy_actions)\n",
    "\n",
    "            selected_actions.append(selected_action)\n",
    "            masks.append(mask)\n",
    "\n",
    "            td.set(\"action\", selected_action)\n",
    "            td = env.step(td)[\"next\"]\n",
    "            step += 1\n",
    "            if step > max_steps:\n",
    "                # log.error(\n",
    "                #     f\"Exceeded maximum number of steps ({max_steps}) duing decoding\"\n",
    "                # )\n",
    "                break\n",
    "\n",
    "\n",
    "        # Output dictionary construction\n",
    "        if calc_reward:\n",
    "            td.set(\"reward\", env.get_reward(td, torch.stack(selected_actions).T))\n",
    "\n",
    "        outdict = {\n",
    "            \"reward\": td[\"reward\"],\n",
    "            \"predicted_q_values\": torch.concat(predicted_q_values, dim=1),\n",
    "            \"masks\": torch.concat(masks, dim=1),\n",
    "            \"done_padding\": torch.concat(done_padding, dim=1)\n",
    "        }\n",
    "\n",
    "        if return_actions:\n",
    "            outdict[\"actions\"] = torch.stack(selected_actions).T\n",
    "\n",
    "        return outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2DModelDQN(DQN):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: RL4COEnvBase,\n",
    "        policy: L2DPolicyDQN = None,\n",
    "        policy_kwargs={},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert env.name in [\n",
    "            \"fjsp\",\n",
    "            \"jssp\",\n",
    "        ], \"L2DModel currently only works for Job-Shop Scheduling Problems\"\n",
    "        if policy is None:\n",
    "            policy = L2DPolicyDQN(env_name=env.name, **policy_kwargs)\n",
    "\n",
    "        super().__init__(env, policy, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4co.envs import FJSPEnv\n",
    "\n",
    "from rl4co.models.nn.graph.hgnn import HetGNNEncoder\n",
    "\n",
    "from rl4co.models.zoo.l2d.decoder import FJSPActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = HetGNNEncoder(embed_dim=32, num_layers=2)\n",
    "decoder = FJSPActor(\n",
    "                    embed_dim=32,\n",
    "                    hidden_dim=64,\n",
    "                    hidden_layers=2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = {\n",
    "  \"num_jobs\": 5,  # the total number of jobs\n",
    "  \"num_machines\": 5,  # the total number of machines that can process operations\n",
    "  \"min_ops_per_job\": 1,  # minimum number of operatios per job\n",
    "  \"max_ops_per_job\": 3,  # maximum number of operations per job\n",
    "  \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation\n",
    "  \"max_processing_time\": 20,  # the maximum time required for a machine to process an operation\n",
    "  \"min_eligible_ma_per_op\": 1,  # the minimum number of machines capable to process an operation\n",
    "  \"max_eligible_ma_per_op\": 2,  # the maximum number of machines capable to process an operation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FJSPEnv(generator_params=generator_params)\n",
    "td = env.reset(batch_size=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 15, 32]), torch.Size([5, 5, 32]))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(op_emb, ma_emb), init = encoder(td)\n",
    "op_emb.shape, ma_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 26]), torch.Size([5, 26]))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values, mask = decoder(td, op_emb, ma_emb)\n",
    "q_values.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0225,  0.0602,  0.0643,  0.0592,  0.0512, -0.0026,  0.0361,  0.0428,\n",
       "         0.0304,  0.0115, -0.0265,  0.0770,  0.0753,  0.0375,  0.0300,  0.0170,\n",
       "         0.0697,  0.0621,  0.0387,  0.0256, -0.0186,  0.0972,  0.1043,  0.0989,\n",
       "         0.0795, -0.0563], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = L2DPolicyDQN(encoder=encoder, decoder=decoder, env_name=\"fjsp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': tensor([-41., -47., -44., -41., -37.]),\n",
       " 'predicted_q_values': tensor([[ 0.0225,  0.0602,  0.0643,  ...,  0.0989,  0.0795, -0.0563],\n",
       "         [ 0.0225,  0.0024, -0.0277,  ...,  0.0151,  0.0247, -0.0510],\n",
       "         [ 0.0225,  0.0836,  0.1845,  ..., -0.0801, -0.0380, -0.1437],\n",
       "         [ 0.0225, -0.0531,  0.0271,  ..., -0.0960, -0.0930, -0.0650],\n",
       "         [ 0.0225,  0.1208,  0.1369,  ..., -0.0022, -0.1089,  0.0940]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 'masks': tensor([[False, False,  True,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]),\n",
       " 'done_padding': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          False,  True],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "           True,  True],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False,  True],\n",
       "         [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "           True,  True]]),\n",
       " 'actions': tensor([[12,  4,  6, 25, 19, 20,  8,  2, 11,  4, 14,  0],\n",
       "         [ 4, 13, 21, 12,  9, 15,  1, 22, 19, 18,  1,  7],\n",
       "         [ 6,  3, 25, 13,  4, 18,  1, 24, 19, 23,  0,  0],\n",
       "         [20,  4,  7, 13, 13, 24,  7, 21, 24, 10, 12,  0],\n",
       "         [10,  3, 24, 11,  3,  8, 18,  0,  0,  0,  0,  0]])}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol(td, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
