{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знакомство с RL4CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from rl4co.envs import FJSPEnv\n",
    "from rl4co.models.zoo.l2d import L2DModel\n",
    "from rl4co.models.zoo.l2d.policy import L2DPolicy\n",
    "from rl4co.models.zoo.l2d.decoder import L2DDecoder\n",
    "from rl4co.models.nn.graph.hgnn import HetGNNEncoder\n",
    "from rl4co.utils.trainer import RL4COTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import FJSPBenchmarksDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FJSPBenchmarksDataset(file_pattern=f\"../data/jsp/benchmarks/*/*.fjs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inst, _ = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = {\n",
    "  \"num_jobs\": 5,  # the total number of jobs\n",
    "  \"num_machines\": 6,  # the total number of machines that can process operations\n",
    "  \"min_ops_per_job\": 1,  # minimum number of operatios per job\n",
    "  \"max_ops_per_job\": 2,  # maximum number of operations per job\n",
    "  \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation\n",
    "  \"max_processing_time\": 20,  # the maximum time required for a machine to process an operation\n",
    "  \"min_eligible_ma_per_op\": 1,  # the minimum number of machines capable to process an operation\n",
    "  \"max_eligible_ma_per_op\": 2,  # the maximum number of machines capable to process an operation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FJSPEnv(generator_params=generator_params)\n",
    "td = env.reset(batch_size=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FJSPEnv()\n",
    "td = env.reset(test_inst.td_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action_mask: Tensor(shape=torch.Size([1, 31]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        busy_until: Tensor(shape=torch.Size([1, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        end_op_per_job: Tensor(shape=torch.Size([1, 5]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        finish_times: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        is_ready: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        job_done: Tensor(shape=torch.Size([1, 5]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        job_in_process: Tensor(shape=torch.Size([1, 5]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        job_ops_adj: Tensor(shape=torch.Size([1, 5, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        lbs: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        ma_assignment: Tensor(shape=torch.Size([1, 6, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next_op: Tensor(shape=torch.Size([1, 5]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        num_eligible: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        op_scheduled: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        ops_adj: Tensor(shape=torch.Size([1, 10, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        ops_job_map: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        ops_ma_adj: Tensor(shape=torch.Size([1, 6, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        ops_sequence_order: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        pad_mask: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        proc_times: Tensor(shape=torch.Size([1, 6, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        start_op_per_job: Tensor(shape=torch.Size([1, 5]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        start_times: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        time: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([1]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 32])\n",
      "torch.Size([1, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "encoder = HetGNNEncoder(embed_dim=32, num_layers=2)\n",
    "(op_emb, ma_emb), init = encoder(td)\n",
    "print(ma_emb.shape)\n",
    "print(op_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2001])\n"
     ]
    }
   ],
   "source": [
    "decoder = L2DDecoder(env_name=env.name, embed_dim=32)\n",
    "logits, mask = decoder(td, (op_emb, ma_emb), num_starts=0)\n",
    "\n",
    "# (1 + num_jobs * num_machines)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step(td):\n",
    "    logits, mask = decoder(td, (op_emb, ma_emb), num_starts=0)\n",
    "    action = logits.masked_fill(~mask, -torch.inf).argmax(1)\n",
    "    td[\"action\"] = action\n",
    "    td = env.step(td)[\"next\"]\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(td, 0)\n",
    "# Update plot within a for loop\n",
    "while not td[\"done\"].all():\n",
    "    # Clear the previous output for the next iteration\n",
    "    # clear_output(wait=True)\n",
    "\n",
    "    td = make_step(td)\n",
    "    # env.render(td, 0)\n",
    "    # Display updated plot\n",
    "    # display(plt.gcf())\n",
    "    \n",
    "    # Pause for a moment to see the changes\n",
    "    # time.sleep(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([624.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    batch_size = 256\n",
    "    train_data_size = 2_000\n",
    "    embed_dim = 128\n",
    "    num_encoder_layers = 4\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    batch_size = 32\n",
    "    train_data_size = 1_000\n",
    "    embed_dim = 64\n",
    "    num_encoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Policy: neural network, in this case with encoder-decoder architecture\n",
    "policy = L2DPolicy(embed_dim=embed_dim, num_encoder_layers=num_encoder_layers, env_name=\"fjsp\")\n",
    "\n",
    "# Model: default is AM with REINFORCE and greedy rollout baseline\n",
    "model = L2DModel(env,\n",
    "                 policy=policy, \n",
    "                 baseline=\"rollout\",\n",
    "                 batch_size=batch_size,\n",
    "                 train_data_size=train_data_size,\n",
    "                 val_data_size=1_000,\n",
    "                 optimizer_kwargs={\"lr\": 1e-4})\n",
    "\n",
    "trainer = RL4COTrainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    logger=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type           | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | env      | FJSPEnv        | 0      | train\n",
      "1 | policy   | L2DPolicy      | 585 K  | train\n",
      "2 | baseline | WarmupBaseline | 585 K  | train\n",
      "----------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.682     Total estimated model params size (MB)\n",
      "124       Modules in train mode\n",
      "120       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f9b7d843d04490a3f8699329445bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edef6dc6884f4e01ad35196a3f1c9c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91351daaf5964afe98c4cfd3412031c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d02cb29b244c6696176511be64e6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1866ea520e4e4a2980b288d2309ed6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81123d91b34c4641a2a7903ed695e0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5765af3d27b54387b18a3eb1f336ebe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7cd0e4ae6544629aa4645bc6868e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b364fa8e344b72a91628d0d067d8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bae974948f345328dbbe3298d4df5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bca29ad3d2c413d9da6229dc6e14bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9234e2573544b09aeb13f129613d0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5642b8e8fc21405190f33b87d1c297cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3735c7c0af24f4e8d513837a4d0692b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4bb1439d7a4a508cf1941585ad7ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d65551fd5548cb9704f068dff2101d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e29abc124214b6385cb7474d19528f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec95a404b416465b8ef61eb330d46702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f672d96c504780a201ea7ca2edb15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0997b6e1b832425a8678096940ffc3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e17f06a726d4cd888edbbf67f7e7dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d924e48ac345a2b85d3a497b1d6b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4f65f3214b460e84763beaab6235a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2970daa73442b7b3c92b7d6e905ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193a975e385c40329d12b6292bd3d991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e92ce775c34382bb1a976cf59f265d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0795e6acf2e840a481ba894b6b8ab4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e068345707e74cb9a6ca93389bc26392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92934f786f44880897aff570da627c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3b60372e98415da9289c970cd7c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88363e44cb943d2b45d26eba25b0391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c3c3817f2f4ad7aa5af0f9ebffd653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.reset(test_inst.td_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 128])\n",
      "torch.Size([1, 500, 128])\n"
     ]
    }
   ],
   "source": [
    "(op_emb, ma_emb), init = model.policy.encoder(td)\n",
    "print(ma_emb.shape)\n",
    "print(op_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_model(td):\n",
    "    logits, mask = model.policy.decoder(td, (op_emb, ma_emb), num_starts=0)\n",
    "    action = logits.masked_fill(~mask, -torch.inf).argmax(1)\n",
    "    td[\"action\"] = action\n",
    "    td = env.step(td)[\"next\"]\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render(td, 0)\n",
    "# Update plot within a for loop\n",
    "while not td[\"done\"].all():\n",
    "    # Clear the previous output for the next iteration\n",
    "    # clear_output(wait=True)\n",
    "\n",
    "    td = make_step_model(td)\n",
    "    # env.render(td, 0)\n",
    "    # # Display updated plot\n",
    "    # display(plt.gcf())\n",
    "    \n",
    "    # # Pause for a moment to see the changes\n",
    "    # time.sleep(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([619.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "source": [
    "policy2 = L2DPolicy(embed_dim=embed_dim, num_encoder_layers=num_encoder_layers, env_name=\"fjsp\")\n",
    "\n",
    "\n",
    "model2 = L2DModel(env,\n",
    "                 policy=policy2, \n",
    "                 baseline=\"rollout\",\n",
    "                 batch_size=batch_size,\n",
    "                 train_data_size=train_data_size,\n",
    "                 val_data_size=1_000,\n",
    "                 optimizer_kwargs={\"lr\": 1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.policy.encoder.init_embedding.init_ops_embed.weight', 'baseline.baseline.policy.encoder.init_embedding.pos_encoder.pe', 'baseline.baseline.policy.encoder.init_embedding.init_ma_embed.weight', 'baseline.baseline.policy.encoder.init_embedding.edge_embed.weight', 'baseline.baseline.policy.encoder.layers.0.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.decoder.actor.dummy', 'baseline.baseline.policy.decoder.actor.mlp.lins.0.weight', 'baseline.baseline.policy.decoder.actor.mlp.lins.0.bias', 'baseline.baseline.policy.decoder.actor.mlp.lins.1.weight', 'baseline.baseline.policy.decoder.actor.mlp.lins.1.bias', 'baseline.baseline.policy.decoder.actor.mlp.lins.2.weight', 'baseline.baseline.policy.decoder.actor.mlp.lins.2.bias']\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "L2DModel(\n",
       "  (env): FJSPEnv()\n",
       "  (policy): L2DPolicy(\n",
       "    (encoder): HetGNNEncoder(\n",
       "      (init_embedding): FJSPInitEmbedding(\n",
       "        (init_ops_embed): Linear(in_features=5, out_features=128, bias=False)\n",
       "        (pos_encoder): PositionalEncoding(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (init_ma_embed): Linear(in_features=1, out_features=128, bias=False)\n",
       "        (edge_embed): Linear(in_features=1, out_features=128, bias=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x HetGNNBlock(\n",
       "          (hgnn1): HetGNNLayer(\n",
       "            (activation): ReLU()\n",
       "          )\n",
       "          (hgnn2): HetGNNLayer(\n",
       "            (activation): ReLU()\n",
       "          )\n",
       "          (ffn1): TransformerFFN(\n",
       "            (ops): ModuleDict(\n",
       "              (norm1): Normalization(\n",
       "                (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (ffn): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "              )\n",
       "              (norm2): Normalization(\n",
       "                (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (ffn2): TransformerFFN(\n",
       "            (ops): ModuleDict(\n",
       "              (norm1): Normalization(\n",
       "                (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (ffn): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "                (1): ReLU()\n",
       "                (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "              )\n",
       "              (norm2): Normalization(\n",
       "                (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): L2DDecoder(\n",
       "      (actor): FJSPActor(\n",
       "        (mlp): MLP(\n",
       "          (hidden_act): ReLU()\n",
       "          (out_act): Identity()\n",
       "          (lins): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "          )\n",
       "          (input_norm): Identity()\n",
       "          (output_norm): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (baseline): WarmupBaseline(\n",
       "    (baseline): RolloutBaseline(\n",
       "      (policy): L2DPolicy(\n",
       "        (encoder): HetGNNEncoder(\n",
       "          (init_embedding): FJSPInitEmbedding(\n",
       "            (init_ops_embed): Linear(in_features=5, out_features=128, bias=False)\n",
       "            (pos_encoder): PositionalEncoding(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (init_ma_embed): Linear(in_features=1, out_features=128, bias=False)\n",
       "            (edge_embed): Linear(in_features=1, out_features=128, bias=False)\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x HetGNNBlock(\n",
       "              (hgnn1): HetGNNLayer(\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (hgnn2): HetGNNLayer(\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "              (ffn1): TransformerFFN(\n",
       "                (ops): ModuleDict(\n",
       "                  (norm1): Normalization(\n",
       "                    (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (ffn): Sequential(\n",
       "                    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "                  )\n",
       "                  (norm2): Normalization(\n",
       "                    (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (ffn2): TransformerFFN(\n",
       "                (ops): ModuleDict(\n",
       "                  (norm1): Normalization(\n",
       "                    (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (ffn): Sequential(\n",
       "                    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "                    (1): ReLU()\n",
       "                    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "                  )\n",
       "                  (norm2): Normalization(\n",
       "                    (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (decoder): L2DDecoder(\n",
       "          (actor): FJSPActor(\n",
       "            (mlp): MLP(\n",
       "              (hidden_act): ReLU()\n",
       "              (out_act): Identity()\n",
       "              (lins): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "              )\n",
       "              (input_norm): Identity()\n",
       "              (output_norm): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (warmup_baseline): ExponentialBaseline()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_from_checkpoint(\"./checkpoints/l2d_checkpoints.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.policy.encoder.init_embedding.init_ops_embed.weight', 'baseline.baseline.policy.encoder.init_embedding.pos_encoder.pe', 'baseline.baseline.policy.encoder.init_embedding.init_ma_embed.weight', 'baseline.baseline.policy.encoder.init_embedding.edge_embed.weight', 'baseline.baseline.policy.encoder.layers.0.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.0.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.1.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.2.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.hgnn1.self_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn1.cross_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn1.edge_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn2.self_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn2.cross_attn', 'baseline.baseline.policy.encoder.layers.3.hgnn2.edge_attn', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.weight', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.bias', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_mean', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_var', 'baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.num_batches_tracked', 'baseline.baseline.policy.decoder.actor.dummy', 'baseline.baseline.policy.decoder.actor.mlp.lins.0.weight', 'baseline.baseline.policy.decoder.actor.mlp.lins.0.bias', 'baseline.baseline.policy.decoder.actor.mlp.lins.1.weight', 'baseline.baseline.policy.decoder.actor.mlp.lins.1.bias', 'baseline.baseline.policy.decoder.actor.mlp.lins.2.weight', 'baseline.baseline.policy.decoder.actor.mlp.lins.2.bias']\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n"
     ]
    }
   ],
   "source": [
    "model3 = L2DModel.load_from_checkpoint(\"./checkpoints/l2d_checkpoints.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = model3.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_model3(td):\n",
    "    logits, mask = model3.policy.decoder(td, (op_emb, ma_emb), num_starts=0)\n",
    "    action = logits.masked_fill(~mask, -torch.inf).argmax(1)\n",
    "    td[\"action\"] = action\n",
    "    td = env.step(td)[\"next\"]\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render(td, 0)\n",
    "# Update plot within a for loop\n",
    "while not td[\"done\"].all():\n",
    "    # Clear the previous output for the next iteration\n",
    "    # clear_output(wait=True)\n",
    "\n",
    "    td = make_step_model3(td)\n",
    "    # env.render(td, 0)\n",
    "    # # Display updated plot\n",
    "    # display(plt.gcf())\n",
    "    \n",
    "    # # Pause for a moment to see the changes\n",
    "    # time.sleep(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([619.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.solvers.jsp.fjsp.rl4co.l2d import FJSPL2DSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = FJSPL2DSolver(train_data_size=train_data_size, batch_size=batch_size, max_epochs=15, accelerator=accelerator, lr=0.0001, val_data_size=1_000, \n",
    "                       embed_dim=embed_dim, num_encoder_layers=num_encoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused keyword arguments: num_jobs, num_machines, min_ops_per_job, max_ops_per_job, min_processing_time, max_processing_time, min_eligible_ma_per_op, max_eligible_ma_per_op. Please check the base class documentation at https://rl4co.readthedocs.io/en/latest/_content/api/envs/base.html. In case you would like to pass data generation arguments, please pass a `generator` method instead or for example: `generator_kwargs=dict(num_loc=50)` to the constructor.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type           | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | env      | FJSPEnv        | 0      | train\n",
      "1 | policy   | L2DPolicy      | 585 K  | train\n",
      "2 | baseline | WarmupBaseline | 585 K  | train\n",
      "----------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.682     Total estimated model params size (MB)\n",
      "124       Modules in train mode\n",
      "120       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1c40d8d0b14c23aed281cd1635ce1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/daniil/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6842a62d1af94d599c88e2adc00ab256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5622fbc180b1446b87de99929c48349b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49606eef9f24cafa2f2a17d10e1a025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00005d85ae94f179fdb5f810cbfe4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a298312638649359247edd104116fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5a207e016a4492a675cbcc283b6d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac788fa12a7045519a289b163407de3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c301a0aefb534d9a92bc1270bcae1211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd42fd086b82486cb3165f1c018aec67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1805ee5e5e47bd814b6f837257c370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764d74588ea1482ba7b6378584636aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dc79d2ae504748b4ef740f8d063ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19a4aeca32e40c0b2e339da02a5dab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6acfac008c34851bbf1164871ab72e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a2e86011a844c49c301b9baa791c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3795ecf6df240eebe0d839082100df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "solver.fit(generator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daniil/programming/diplom/mtrlgnn/notebooks/lightning_logs/version_6/checkpoints/epoch=14-step=120.ckpt'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FJSPL2DSolver:\n\tUnexpected key(s) in state_dict: \"baseline.baseline.policy.encoder.init_embedding.init_ops_embed.weight\", \"baseline.baseline.policy.encoder.init_embedding.pos_encoder.pe\", \"baseline.baseline.policy.encoder.init_embedding.init_ma_embed.weight\", \"baseline.baseline.policy.encoder.init_embedding.edge_embed.weight\", \"baseline.baseline.policy.encoder.layers.0.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.decoder.actor.dummy\", \"baseline.baseline.policy.decoder.actor.mlp.lins.0.weight\", \"baseline.baseline.policy.decoder.actor.mlp.lins.0.bias\", \"baseline.baseline.policy.decoder.actor.mlp.lins.1.weight\", \"baseline.baseline.policy.decoder.actor.mlp.lins.1.bias\", \"baseline.baseline.policy.decoder.actor.mlp.lins.2.weight\", \"baseline.baseline.policy.decoder.actor.mlp.lins.2.bias\", \"policy.encoder.layers.1.hgnn1.self_attn\", \"policy.encoder.layers.1.hgnn1.cross_attn\", \"policy.encoder.layers.1.hgnn1.edge_attn\", \"policy.encoder.layers.1.hgnn2.self_attn\", \"policy.encoder.layers.1.hgnn2.cross_attn\", \"policy.encoder.layers.1.hgnn2.edge_attn\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.weight\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.bias\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.1.ffn1.ops.ffn.0.weight\", \"policy.encoder.layers.1.ffn1.ops.ffn.0.bias\", \"policy.encoder.layers.1.ffn1.ops.ffn.2.weight\", \"policy.encoder.layers.1.ffn1.ops.ffn.2.bias\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.weight\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.bias\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.weight\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.bias\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.1.ffn2.ops.ffn.0.weight\", \"policy.encoder.layers.1.ffn2.ops.ffn.0.bias\", \"policy.encoder.layers.1.ffn2.ops.ffn.2.weight\", \"policy.encoder.layers.1.ffn2.ops.ffn.2.bias\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.weight\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.bias\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.hgnn1.self_attn\", \"policy.encoder.layers.2.hgnn1.cross_attn\", \"policy.encoder.layers.2.hgnn1.edge_attn\", \"policy.encoder.layers.2.hgnn2.self_attn\", \"policy.encoder.layers.2.hgnn2.cross_attn\", \"policy.encoder.layers.2.hgnn2.edge_attn\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.weight\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.bias\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.ffn1.ops.ffn.0.weight\", \"policy.encoder.layers.2.ffn1.ops.ffn.0.bias\", \"policy.encoder.layers.2.ffn1.ops.ffn.2.weight\", \"policy.encoder.layers.2.ffn1.ops.ffn.2.bias\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.weight\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.bias\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.weight\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.bias\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.ffn2.ops.ffn.0.weight\", \"policy.encoder.layers.2.ffn2.ops.ffn.0.bias\", \"policy.encoder.layers.2.ffn2.ops.ffn.2.weight\", \"policy.encoder.layers.2.ffn2.ops.ffn.2.bias\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.weight\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.bias\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.hgnn1.self_attn\", \"policy.encoder.layers.3.hgnn1.cross_attn\", \"policy.encoder.layers.3.hgnn1.edge_attn\", \"policy.encoder.layers.3.hgnn2.self_attn\", \"policy.encoder.layers.3.hgnn2.cross_attn\", \"policy.encoder.layers.3.hgnn2.edge_attn\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.weight\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.bias\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.ffn1.ops.ffn.0.weight\", \"policy.encoder.layers.3.ffn1.ops.ffn.0.bias\", \"policy.encoder.layers.3.ffn1.ops.ffn.2.weight\", \"policy.encoder.layers.3.ffn1.ops.ffn.2.bias\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.weight\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.bias\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.weight\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.bias\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.ffn2.ops.ffn.0.weight\", \"policy.encoder.layers.3.ffn2.ops.ffn.0.bias\", \"policy.encoder.layers.3.ffn2.ops.ffn.2.weight\", \"policy.encoder.layers.3.ffn2.ops.ffn.2.bias\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.weight\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.bias\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.num_batches_tracked\". \n\tsize mismatch for policy.encoder.init_embedding.init_ops_embed.weight: copying a param with shape torch.Size([128, 5]) from checkpoint, the shape in current model is torch.Size([32, 5]).\n\tsize mismatch for policy.encoder.init_embedding.pos_encoder.pe: copying a param with shape torch.Size([1, 1000, 128]) from checkpoint, the shape in current model is torch.Size([1, 1000, 32]).\n\tsize mismatch for policy.encoder.init_embedding.init_ma_embed.weight: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.init_embedding.edge_embed.weight: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn1.self_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn1.cross_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn1.edge_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn2.self_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn2.cross_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn2.edge_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.decoder.actor.dummy: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.0.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.1.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m solver2 \u001b[38;5;241m=\u001b[39m \u001b[43mFJSPL2DSolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:187\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    184\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FJSPL2DSolver:\n\tUnexpected key(s) in state_dict: \"baseline.baseline.policy.encoder.init_embedding.init_ops_embed.weight\", \"baseline.baseline.policy.encoder.init_embedding.pos_encoder.pe\", \"baseline.baseline.policy.encoder.init_embedding.init_ma_embed.weight\", \"baseline.baseline.policy.encoder.init_embedding.edge_embed.weight\", \"baseline.baseline.policy.encoder.layers.0.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.0.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.0.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.1.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.1.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.2.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.2.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.hgnn1.self_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn1.cross_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn1.edge_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn2.self_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn2.cross_attn\", \"baseline.baseline.policy.encoder.layers.3.hgnn2.edge_attn\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.0.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.ffn.2.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.weight\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.bias\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_mean\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_var\", \"baseline.baseline.policy.encoder.layers.3.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"baseline.baseline.policy.decoder.actor.dummy\", \"baseline.baseline.policy.decoder.actor.mlp.lins.0.weight\", \"baseline.baseline.policy.decoder.actor.mlp.lins.0.bias\", \"baseline.baseline.policy.decoder.actor.mlp.lins.1.weight\", \"baseline.baseline.policy.decoder.actor.mlp.lins.1.bias\", \"baseline.baseline.policy.decoder.actor.mlp.lins.2.weight\", \"baseline.baseline.policy.decoder.actor.mlp.lins.2.bias\", \"policy.encoder.layers.1.hgnn1.self_attn\", \"policy.encoder.layers.1.hgnn1.cross_attn\", \"policy.encoder.layers.1.hgnn1.edge_attn\", \"policy.encoder.layers.1.hgnn2.self_attn\", \"policy.encoder.layers.1.hgnn2.cross_attn\", \"policy.encoder.layers.1.hgnn2.edge_attn\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.weight\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.bias\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.1.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.1.ffn1.ops.ffn.0.weight\", \"policy.encoder.layers.1.ffn1.ops.ffn.0.bias\", \"policy.encoder.layers.1.ffn1.ops.ffn.2.weight\", \"policy.encoder.layers.1.ffn1.ops.ffn.2.bias\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.weight\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.bias\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.1.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.weight\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.bias\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.1.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.1.ffn2.ops.ffn.0.weight\", \"policy.encoder.layers.1.ffn2.ops.ffn.0.bias\", \"policy.encoder.layers.1.ffn2.ops.ffn.2.weight\", \"policy.encoder.layers.1.ffn2.ops.ffn.2.bias\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.weight\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.bias\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.1.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.hgnn1.self_attn\", \"policy.encoder.layers.2.hgnn1.cross_attn\", \"policy.encoder.layers.2.hgnn1.edge_attn\", \"policy.encoder.layers.2.hgnn2.self_attn\", \"policy.encoder.layers.2.hgnn2.cross_attn\", \"policy.encoder.layers.2.hgnn2.edge_attn\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.weight\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.bias\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.2.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.ffn1.ops.ffn.0.weight\", \"policy.encoder.layers.2.ffn1.ops.ffn.0.bias\", \"policy.encoder.layers.2.ffn1.ops.ffn.2.weight\", \"policy.encoder.layers.2.ffn1.ops.ffn.2.bias\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.weight\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.bias\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.2.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.weight\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.bias\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.2.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.2.ffn2.ops.ffn.0.weight\", \"policy.encoder.layers.2.ffn2.ops.ffn.0.bias\", \"policy.encoder.layers.2.ffn2.ops.ffn.2.weight\", \"policy.encoder.layers.2.ffn2.ops.ffn.2.bias\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.weight\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.bias\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.2.ffn2.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.hgnn1.self_attn\", \"policy.encoder.layers.3.hgnn1.cross_attn\", \"policy.encoder.layers.3.hgnn1.edge_attn\", \"policy.encoder.layers.3.hgnn2.self_attn\", \"policy.encoder.layers.3.hgnn2.cross_attn\", \"policy.encoder.layers.3.hgnn2.edge_attn\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.weight\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.bias\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.3.ffn1.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.ffn1.ops.ffn.0.weight\", \"policy.encoder.layers.3.ffn1.ops.ffn.0.bias\", \"policy.encoder.layers.3.ffn1.ops.ffn.2.weight\", \"policy.encoder.layers.3.ffn1.ops.ffn.2.bias\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.weight\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.bias\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.3.ffn1.ops.norm2.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.weight\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.bias\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_mean\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.running_var\", \"policy.encoder.layers.3.ffn2.ops.norm1.normalizer.num_batches_tracked\", \"policy.encoder.layers.3.ffn2.ops.ffn.0.weight\", \"policy.encoder.layers.3.ffn2.ops.ffn.0.bias\", \"policy.encoder.layers.3.ffn2.ops.ffn.2.weight\", \"policy.encoder.layers.3.ffn2.ops.ffn.2.bias\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.weight\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.bias\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_mean\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.running_var\", \"policy.encoder.layers.3.ffn2.ops.norm2.normalizer.num_batches_tracked\". \n\tsize mismatch for policy.encoder.init_embedding.init_ops_embed.weight: copying a param with shape torch.Size([128, 5]) from checkpoint, the shape in current model is torch.Size([32, 5]).\n\tsize mismatch for policy.encoder.init_embedding.pos_encoder.pe: copying a param with shape torch.Size([1, 1000, 128]) from checkpoint, the shape in current model is torch.Size([1, 1000, 32]).\n\tsize mismatch for policy.encoder.init_embedding.init_ma_embed.weight: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.init_embedding.edge_embed.weight: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn1.self_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn1.cross_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn1.edge_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn2.self_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn2.cross_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.hgnn2.edge_attn: copying a param with shape torch.Size([128, 1]) from checkpoint, the shape in current model is torch.Size([32, 1]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm1.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.ffn.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn1.ops.norm2.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm1.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([64, 32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.ffn.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.encoder.layers.0.ffn2.ops.norm2.normalizer.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.decoder.actor.dummy: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.0.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.1.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([32, 32]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy.decoder.actor.mlp.lins.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 32])."
     ]
    }
   ],
   "source": [
    "solver2 = FJSPL2DSolver.load_from_checkpoint(solver.save_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL rl4co.envs.scheduling.fjsp.env.FJSPEnv was not an allowed global by default. Please use `torch.serialization.add_safe_globals([FJSPEnv])` or the `torch.serialization.safe_globals([FJSPEnv])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(checkpoint\u001b[38;5;241m.\u001b[39mkeys())  \u001b[38;5;66;03m# Проверьте наличие 'hyper_parameters'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyper_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Какие параметры там сохранены\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/diplom/mtrlgnn/.venv/lib/python3.10/site-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL rl4co.envs.scheduling.fjsp.env.FJSPEnv was not an allowed global by default. Please use `torch.serialization.add_safe_globals([FJSPEnv])` or the `torch.serialization.safe_globals([FJSPEnv])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(solver.save_model(), map_location='cpu')\n",
    "print(checkpoint.keys())  # Проверьте наличие 'hyper_parameters'\n",
    "print(checkpoint['hyper_parameters'])  # Какие параметры там сохранены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(solver.policy, \"l2d.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
